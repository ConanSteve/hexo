---
title: Pytorch
date: 2022-02-01 10:00:00
author: 陌上人如玉
categories: Pytorch
---

# 环境安装

## windows

[PyTorch环境配置及安装](https://www.cnblogs.com/zhouzhiyao/p/11784055.html)

```
conda install pytorch torchvision torchaudio torchtext cudatoolkit=10.1 -c pytorch
```

[windows下安装anaconda+pytorch1.0+cuda10+配置pycharm](https://blog.csdn.net/ha_ha_ha233/article/details/87475799)

[windows10安装cuda](https://www.cnblogs.com/arxive/p/11198420.html)

> 显卡驱动版本一定要大于cudatoolkit版本
> [Ubuntu16.04下cuda和cudnn的卸载和升级和环境配置](https://blog.csdn.net/wanzhen4330/article/details/81704474)
>  用pip卸载，conda卸载不干净
> 查看pytorch对应的cuda版本

## Mac：

[Mac版本安装Anaconda及使用教程](https://blog.csdn.net/qq_38105596/article/details/100926386)

[Macbook air m1安装python/anaconda全过程(图文)](https://www.jb51.net/article/208315.htm)

[Mac M1安装anaconda与pycharm与pytorch与jupyter](https://blog.csdn.net/weixin_42438346/article/details/120351207)

`torch.version.cuda`

# 基础知识

[batch_size的理解](https://blog.csdn.net/qq_42380515/article/details/87885996)

[PyTorch之前向传播函数forward](https://blog.csdn.net/u011501388/article/details/84062483)

## 参考资料

[简书笔记01](https://www.jianshu.com/u/b3c66a77e742)

# 接口

[torch.unsqueeze()](https://blog.csdn.net/xiexu911/article/details/80820028)

[Dataloader重要参数与内部机制](https://blog.csdn.net/zyq12345678/article/details/90268668)

## torch.nn

[PyTorch之前向传播函数forward](https://blog.csdn.net/u011501388/article/details/84062483)

[touch.nn.module](https://zhuanlan.zhihu.com/p/340453841)

[forward](https://blog.csdn.net/u011501388/article/details/84062483)

[torch.nn.Sequential()](https://blog.csdn.net/dss_dssssd/article/details/82980222)

[torch.nn.Parameter()](https://www.jianshu.com/p/d8b77cc02410)

> 就是将Tensor变成Variable

[nn.Embedding](https://www.jianshu.com/p/63e7acc5e890)

输入维度：batchSize * SeqLen

输出维度：batchSize * SeqLen * EmbeddingDim

[torch.nn.LSTM()详解](https://blog.csdn.net/m0_45478865/article/details/104455978)

输入维度

[PyTorch中的nn.Conv1d与nn.Conv2d](https://www.jianshu.com/p/45a26d278473)

## torch.optim.lr_scheduler

[lr_scheduler.StepLR调整学习率机制](https://blog.csdn.net/weixin_42305378/article/details/108740926)

[torch.optim.lr_scheduler：调整学习率](https://blog.csdn.net/qyhaill/article/details/103043637)

## 优化器
[简单认识Adam优化器](https://zhuanlan.zhihu.com/p/32698042)

## loss

### nllloss

[Pytorch详解NLLLoss和CrossEntropyLoss](https://blog.csdn.net/qq_22210253/article/details/85229988)

```Python
import torch, torch.nn as nn
import torch.nn.functional as F

torch.random.manual_seed(2020)
input = torch.randn(4,3)
print(input)

sm = nn.Softmax(dim =0)
x0=sm(input)
print(x0)

sm = nn.Softmax(dim =1)
x1=sm(input)
print(x1)

x2 = torch.log(x1)
print(x2)

target = torch.tensor([0,2,1,2])
print(-(x2[0,0]+x2[1,2]+x2[2,1]+x2[3,2])/4)
print(F.nll_loss(x2, target))
print(F.nll_loss(x2, target, size_average = False))
print(F.nll_loss(x2, target, reduction = 'sum'))

# CrossEntropyLoss就是把以上Softmax–Log–NLLLoss合并成一步
loss = nn.CrossEntropyLoss()
print(loss(input, target))
```

## transformers

[官方文档](https://huggingface.co/transformers/)

[Pytorch：transforms的二十二个方法](https://blog.csdn.net/weixin_38533896/article/details/86028509)

# 包

## Stanford parser

[NLP工具——stanford Parser使用手册](https://blog.csdn.net/u010454729/article/details/88358932)

[2](https://blog.csdn.net/l919898756/article/details/81670228)

[（Java）利用Stanford parser与多线程获取语句中名词集合工具实现](https://blog.csdn.net/qq_41733192/article/details/106589613)

[Stanford Parser 标记含义](https://www.cnblogs.com/supakito/archive/2012/12/12/2814121.html)

```Python
from stanfordcorenlp import StanfordCoreNLP
path = '../stanford-corenlp-4.2.2'
nlp = StanfordCoreNLP(path, lang='en')
s = 'Stanford University is located in California. It is a great university, founded in 1891.'
 
token = nlp.word_tokenize(s)
postag = nlp.pos_tag(s)
ner = nlp.ner(s)
parse = nlp.parse(s)
dependencyParse = nlp.dependency_parse(s)
 
print(' '.join(token))
print('|'.join([','.join(i) for i in postag]))
print('|'.join([','.join(i) for i in ner]))
print(parse)
for i, begin, end in dependencyParse:
    print(i, '-'.join([str(begin), token[begin-1]]), '-'.join([str(end),token[end-1]]))

nlp.close()
```

## torchcrf

[pytorch-crf的使用](https://www.jianshu.com/p/cc4c3ae9b762)

```Python
from torchcrf import CRF
pip install pytorch-crf==0.4.0
```



# 训练

[torch.nn.DataParallel](https://zhuanlan.zhihu.com/p/102697821)

Cuda out of memory

> ```
> os.environ["CUDA_VISIBLE_DEVICES"] = '3,4,5'
> ```

 [Pytorch（五）入门：DataLoader 和 Dataset](https://blog.csdn.net/zw__chen/article/details/82806900)

[多GPU训练](https://blog.csdn.net/pearl8899/article/details/109567745)

###### Batch

batch size过小，花费时间多，同时梯度震荡严重，不利于收敛；batch size过大，不同batch的梯度方向没有任何变化，容易陷入局部极小值。

# 经典模型


# 评估

[NLP（二十三）序列标注算法评估模块seqeval的使用 - 山阴少年 - 博客园 (cnblogs.com)](https://www.cnblogs.com/jclian91/p/12459688.html)

Macro-F1: macro f1需要先计算出每一个类别的准召及其f1 score，然后通过求均值得到在整个样本上的f1 score.

Micro-F1: micro f1不需要区分类别，直接使用总体样本的准召计算f1 score.

Weighted-F1: 在macro f1基础上，考虑 每一类别 的个数，进行加权平均。

[micro-f1 & macro-f1 & weighted-f1](https://blog.csdn.net/Zhou_Dao/article/details/105332981)

# 常用

[Anaconda设置CUDA版本和系统默认版本共存](https://blog.csdn.net/qq_42189368/article/details/109361785)

[pandas](https://www.jianshu.com/p/840ba135df30)

[获取当前目录路径和文件](https://www.cnblogs.com/Jomini/p/8636129.html)

 [python标准库之glob介绍](https://www.cnblogs.com/luminousjj/p/9359543.html)

 [读取excel文件库xlrd](https://www.cnblogs.com/insane-Mr-Li/p/9092619.html)

 [写excel库xlwt](https://www.cnblogs.com/TestSu/p/10084488.html)

 [Python操作excel：用xlwt设置excel单元格背景颜色，给字体加粗](https://blog.csdn.net/qq_38161040/article/details/89553356)

[读写csv](https://www.cnblogs.com/unnameable/p/7366437.html)

```Python
import csv
ine_num=2
file_path = ""
with open(file_path, 'r') as f:
    reader = csv.reader(f)
    print(type(reader))    
    i=0
    for row in reader:
        i+=1
        print(row)
        if i==ine_num:
            break
```



# TEMP

[torch.Tensor.detach()](https://zhuanlan.zhihu.com/p/389738863)

[torch.cat()](https://blog.csdn.net/xinjieyuan/article/details/105208352)

[从ReLU到GELU，一文概览神经网络的激活函数](https://baijiahao.baidu.com/s?id=1653421414340022957&wfr=spider&for=pc)

# 项目实战

## 文本分类

[NLP 找门：用朴素贝叶斯进行文本分类](https://flyhigher.top/develop/1989.html)

[BERT文本分类](https://www.cnblogs.com/wwj99/p/12283799.html)

[PyTorch环境下对BERT进行Fine-tuning](https://blog.csdn.net/sinat_20829389/article/details/110730570?utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.essearch_pc_relevant&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.essearch_pc_relevant)

[文本分类tensorflow](https://www.cnblogs.com/jiangxinyang/p/10210813.html)



## Others