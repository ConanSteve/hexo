---
title: batchsize太小的缺点 随着batchsize逐渐增大的优缺点 如何平衡batchsize的大小
date: 2022-07-12
author: 陌上人如玉
comments:
description:
keywords:
top_img:
cover:
mathjax:
katex:
aside:
aplayer:
highlight_shrink:
tags: 
categories: Pytorch
---

# 一、定义
简单点说，就是我们一次要将多少个数据扔进模型去训练，这个值介于1和训练样本总个数之间。

# 二、若batchsize太小的缺点
①耗时长，训练效率低。
假设batchsize=1，每次用一个数据进行训练，如果数据总量很多时(假设有十万条数据），就需要向模型投十万次数据，完整训练完一遍数据需要很长的时问，训练效率很低；
②训练数据就会非常难收敛，从而导致欠拟合。
假设batchsize=1，每次用一个数据进行训练，则由于个体的差异性或者异常值的影响，模型的参数变化也会很大，每一层的梯度都具有很高的随机性，而且需要耗费了大量的时间，从而导致模型非常难收敛。

# 三、随着batchsize逐渐增大的优缺点
1.大的batchsize减少训练时间的同时所需内存容量增加
①大的batchsize减少训练时间
这是肯定的，同样的epoch数目，大的batchsize需要的batch数目减少了，所以处理速度变快，可以减少训练时间；
②大的batchsize所需内存容量增加
但是如果该值太大，假设batchsize=100000，一次将十万条数据扔进模型，很可能会造成内存溢出，而无法正常进行训练。

2.大的batchsize在提高稳定性的同时可能导致模型泛化能力下降
①大的batch size梯度的计算更加稳定
因为模型训练曲线会更加平滑。在微调的时候，大的batchsize可能会取得更好的结果。因为在一定范围内，一般来说batchsize越大，其确定的下降方向越准，引起训练震荡越小。batchsize增大到一定程度，其确定的下降方向已经基本不再变化。
②大的batchsize可能导致模型泛化能力下降
在一定范围内，增加batchsize有助于收敛的稳定性，但是随着batchsize的增加，模型的泛化性能会下降。若batchsize设为最大（样本总个数），则每次更新参数都是相同的样本，下降方向基本确定，这会导致模型的泛化性能下降。

# 四、如何平衡batchsize的大小？
batchsize太大或者太小都不好。所以 batch size 的值越大，梯度也就越稳定，而 batch size 越小，梯度具有越高的随机性，但如果 batch size 太大，对于内存的需求就更高，同时也不利于网络跳出局部极小点。所以，我们需要设置一个合适的batchsize值，在训练速度和内存容量之间寻找到最佳的平衡点。
①一般在Batchsize增加的同时，我们需要对所有样本的训练次数（也就是后面要讲的epoch）增加（以增加训练次数达到更好的效果）这同样会导致耗时增加，因此需要寻找一个合适的batchsize值，在模型总体效率和内存容量之间做到最好的平衡。
②由于上述两种因素的矛盾，batchsize增大到某个时候，达到时间上的最优。由于最终收敛精度会陷入不同的局部极值，因此batchsize增大到某些时候，达到最终收敛精度上的最优。